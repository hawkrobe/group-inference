{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### process the final_raw_noauto modeling output into formats that could be easily read into R\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tomtom_final_raw_noauto_data4viz.pkl','rb') as f:\n",
    "    [hmeans,smeans,dmeans,stor_grp,stor_grp_prb,stor_dim_prb,grp_map,dim_map] = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # massage from 9,15,4 array of 15,4 tensors to 9,15,4,15,4,\n",
    "# def torch2np(mat):\n",
    "#     newsize = list(mat.shape)\n",
    "#     newsize.extend(list(mat[0,0,0].shape))\n",
    "#     newmat = np.empty(shape = newsize)\n",
    "#     enu = np.ndenumerate(mat)\n",
    "#     for i in enu:\n",
    "#         ind = i[0]\n",
    "#         val = i[1]\n",
    "#         newmat[ind] = val.detach().numpy()\n",
    "#     return newmat\n",
    "# hmeans = torch2np(hmeans)\n",
    "# smeans = torch2np(smeans)\n",
    "# dmeans = torch2np(dmeans)\n",
    "\n",
    "# def dicttensor2dictarray(dic):\n",
    "#     for i in dic.keys():\n",
    "#         dic[i] = dic[i].detach().numpy()\n",
    "#     return dic\n",
    "# grp_map = dicttensor2dictarray(grp_map)\n",
    "# dim_map = dicttensor2dictarray(dim_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### extract components\n",
    "\n",
    "extract each component from both group and dimension models, save to separate csvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "[dim_comp_1, dim_comp_2, dim_comp_3] = [\n",
    "    torch.distributions.Beta(dim_map['topic_a'][i],dim_map['topic_b'][i]).mean.detach() for i in range(len(dim_map['topic_a']))\n",
    "]\n",
    "\n",
    "[grp_comp_1, grp_comp_2, grp_comp_3] = [\n",
    "    torch.distributions.Beta(grp_map['alpha'][i],grp_map['beta'][i]).mean.detach() for i in range(len(grp_map['alpha']))\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### classification/dimension weights to csv\n",
    "\n",
    "save input-classification/weight associations to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "enu = np.ndenumerate(stor_grp)\n",
    "full = []\n",
    "for ele in enu:\n",
    "    i,j,k = ele[0]\n",
    "    grp_h = ele[1]\n",
    "    grp_s = stor_grp_prb[i,j,k].detach().numpy()\n",
    "    dim = stor_dim_prb[i,j,k].detach().numpy()\n",
    "    row = [i,j,k,grp_h]\n",
    "    row.extend(grp_s)\n",
    "    row.extend(dim)\n",
    "    full.append(row)\n",
    "\n",
    "colnames = ['step','from','to','grp_h']\n",
    "[colnames.append('grp_s_{}'.format(i + 1)) for i in range(3)]\n",
    "[colnames.append('dim_{}'.format(i + 1)) for i in range(3)]\n",
    "df = pd.DataFrame(full, columns = colnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in states\n",
    "states = pd.read_csv('C:/Users/zhaoz/group-inference/data/states.csv') # there's probably a better place to put this\n",
    "states['statepair'] = states['state1'] + '_' + states['state2']# construct state pair strings\n",
    "# remove autotransiitons\n",
    "states_noauto = states.loc[states.state1 != states.state2].reset_index(drop = True)\n",
    "# repeat 9 times to match up with the full dataframe\n",
    "nrep = df.shape[0]//states_noauto.shape[0]\n",
    "expanded_states = states_noauto.copy()\n",
    "for i in range(nrep-1):\n",
    "    expanded_states = expanded_states.append(states_noauto)\n",
    "\n",
    "# first merge with the main frame\n",
    "df = pd.concat([df.reset_index(drop = True),expanded_states.reset_index(drop = True)], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge with original states to form final data frame. auto transitions all columns coded -1\n",
    "temps = []\n",
    "for i in df.step.unique():\n",
    "    temp = df[df.step == i]\n",
    "    temp_wstate = states.merge(\n",
    "        temp,how = 'left',on = ['state1','state2','set','statepair']\n",
    "    )\n",
    "    # fill the NAs in step\n",
    "    temp_wstate['step'] = i\n",
    "    temps.append(temp_wstate)\n",
    "    \n",
    "df = pd.concat(temps,axis = 0).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to file\n",
    "df.to_csv('infer_unobserved.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save components in long format after adding state information\n",
    "def process_component(comp):\n",
    "    a = pd.DataFrame(comp.detach().numpy()).melt(ignore_index = False,var_name = 'to')\n",
    "    a['from'] = a.index\n",
    "    a = a.sort_values(['from','to'])\n",
    "    a = pd.concat([a.reset_index(drop = True),states_noauto.reset_index(drop = True)], axis = 1)\n",
    "    b = states.merge(\n",
    "        a,how = 'left',on = ['state1','state2','set','statepair']\n",
    "    )\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_comp_1 = process_component(dim_comp_1)\n",
    "dim_comp_2 = process_component(dim_comp_2)\n",
    "dim_comp_3 = process_component(dim_comp_3)\n",
    "grp_comp_1 = process_component(grp_comp_1)\n",
    "grp_comp_2 = process_component(grp_comp_2)\n",
    "grp_comp_3 = process_component(grp_comp_3)\n",
    "\n",
    "dim_comp_1.to_csv('dim_comp_1.csv', index = False) \n",
    "dim_comp_2.to_csv('dim_comp_2.csv', index = False) \n",
    "dim_comp_3.to_csv('dim_comp_3.csv', index = False) \n",
    "grp_comp_1.to_csv('grp_comp_1.csv', index = False) \n",
    "grp_comp_2.to_csv('grp_comp_2.csv', index = False) \n",
    "grp_comp_3.to_csv('grp_comp_3.csv', index = False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
