{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "from torch.distributions import constraints\n",
    "from matplotlib import pyplot\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro import poutine\n",
    "from pyro.infer.autoguide import AutoDelta\n",
    "from pyro.optim import Adam\n",
    "from pyro.infer import SVI, TraceEnum_ELBO, config_enumerate, infer_discrete, Predictive\n",
    "from pyro.ops.indexing import Vindex\n",
    "from pyro.infer import MCMC, NUTS\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([  7,  12,  13,  38,  42,  55,  56,  57,  58,  60,  73,  77,  81,\n",
      "        83,  87,  91,  95,  97,  98,  99, 101, 103, 114, 117, 119, 123,\n",
      "       125, 126, 128, 131, 133, 138, 141, 146, 147, 152, 155, 157, 159,\n",
      "       170, 190, 196, 199, 201, 202, 209, 214, 220, 225, 226, 232, 237,\n",
      "       238, 245, 247, 257, 264, 271, 275, 277, 291, 293, 294, 298, 301,\n",
      "       303, 322, 323, 334, 340, 341]), array([  2,   9,  10,  14,  17,  18,  25,  27,  28,  33,  43,  50,  52,\n",
      "        54,  62,  63,  68,  72,  85,  86, 106, 107, 108, 110, 116, 139,\n",
      "       145, 160, 164, 165, 166, 174, 175, 178, 184, 185, 187, 191, 192,\n",
      "       195, 198, 203, 207, 208, 210, 212, 213, 215, 219, 223, 228, 236,\n",
      "       250, 251, 252, 254, 255, 260, 263, 265, 285, 295, 311, 321, 325,\n",
      "       330, 332, 336, 337, 345, 350]), array([  0,   1,   4,   5,  15,  19,  20,  24,  30,  32,  34,  36,  40,\n",
      "        48,  51,  61,  67,  69,  70,  76,  79,  89,  90,  93, 100, 134,\n",
      "       135, 137, 143, 151, 153, 156, 167, 171, 173, 182, 189, 194, 197,\n",
      "       204, 205, 217, 230, 234, 240, 241, 243, 248, 249, 258, 261, 262,\n",
      "       266, 272, 280, 282, 286, 288, 297, 305, 308, 314, 315, 318, 320,\n",
      "       327, 331, 333, 342, 346, 347]), array([  6,  11,  16,  21,  23,  37,  39,  49,  59,  64,  65,  66,  75,\n",
      "        78,  82,  84,  96, 105, 112, 120, 122, 132, 144, 148, 149, 168,\n",
      "       169, 176, 179, 181, 183, 193, 206, 211, 218, 229, 233, 235, 242,\n",
      "       244, 246, 256, 267, 269, 270, 273, 274, 276, 281, 283, 284, 289,\n",
      "       290, 296, 299, 306, 309, 310, 312, 313, 317, 319, 324, 326, 328,\n",
      "       329, 335, 339, 343, 352]), array([  3,   8,  22,  26,  29,  31,  35,  41,  44,  45,  46,  47,  53,\n",
      "        71,  74,  80,  88,  92,  94, 102, 104, 109, 111, 113, 115, 118,\n",
      "       121, 124, 127, 129, 130, 136, 140, 142, 150, 154, 158, 161, 162,\n",
      "       163, 172, 177, 180, 186, 188, 200, 216, 221, 222, 224, 227, 231,\n",
      "       239, 253, 259, 268, 278, 279, 287, 292, 300, 302, 304, 307, 316,\n",
      "       338, 344, 348, 349, 351])]\n"
     ]
    }
   ],
   "source": [
    "with open('tomtom_data_preprocessed_withadded.pkl','rb') as f:\n",
    "    [tself_norm_all_3d, tself_norm_noauto_3d, tself_raw_all_3d, tself_raw_noauto_3d,\n",
    "    ttarg_norm_all_3d, ttarg_norm_noauto_3d, ttarg_raw_all_3d, ttarg_raw_noauto_3d,\n",
    "    tavg_norm_all_3d, tavg_norm_noauto_3d, tavg_raw_all_3d, tavg_raw_noauto_3d] = pickle.load(f)\n",
    "\n",
    "kf = KFold(n_splits = 5, shuffle = True, random_state = 888) # make sure these parameters match up with what's used to train/test\n",
    "\n",
    "testinds = []\n",
    "for train, test in kf.split(list(range(tself_raw_noauto_3d.shape[0]))):\n",
    "    testinds.append(test)\n",
    "    \n",
    "# print(testinds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # read in metrics and put them in a df\n",
    "# maxk = 10\n",
    "\n",
    "# allrows = []\n",
    "# for k in range(maxk):  \n",
    "#     with open('kfold_metrics_grp_k{}.pkl'.format(k+1),'rb') as f:\n",
    "#         [ink] = pickle.load(f)\n",
    "#         # each element in ink is results from a split\n",
    "#         for split_id in range(len(ink)):\n",
    "#             true_ids = testinds[split_id]\n",
    "#             split = ink[split_id]\n",
    "#             metric_list = [metric for metric in split]\n",
    "#             [npar,nstimfrom,nstimto] = list(split[0].shape)\n",
    "#             for participant in range(npar):\n",
    "#                 for stimfrom in range(nstimfrom):\n",
    "#                     for stimto in range(nstimto):\n",
    "#                         pred_list = [metric[participant,stimfrom,stimto] for metric in split]\n",
    "#                         [npredfrom, npredto] = list(pred_list[0].shape)\n",
    "#                         for predfrom in range(npredfrom):\n",
    "#                             for predto in range(npredto):\n",
    "#                                 single_row = [k+1,true_ids[participant], stimfrom, stimto, predfrom,predto]\n",
    "#                                 for i in range(len(pred_list)):\n",
    "#                                     if i == (len(pred_list) - 1): # turns out MixtureSameFamily only returns the total log prob of evertying in the distribution object, so there's no by-item lp for soft (or dimensional)\n",
    "#                                         single_row.append(pred_list[i].item()/(npredfrom*npredto)) #spread the total logprob across cells to make summing later easier\n",
    "#                                     else:    \n",
    "#                                         single_row.append(pred_list[i][predfrom,predto].item())\n",
    "#                                 allrows.append(np.array(single_row))\n",
    "#     del ink\n",
    "                                \n",
    "# allpred = np.stack(allrows,axis = 0)\n",
    "# dfgrp = pd.DataFrame(allpred, columns = ['K','pid','stimfrom','stimto','predfrom','predto','h_ae','s_ae','h_se','s_se','h_lp','s_lpX'])\n",
    "# dfgrp.to_csv('kfold_metrics_grp.csv',index = False)     \n",
    "\n",
    "# # release memory \n",
    "# del allrows\n",
    "                \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "allrows = []\n",
    "maxk = 10\n",
    "for k in range(maxk):\n",
    "    for split_id in range(5):       \n",
    "        with open('kfold_metrics_dim_k{}_split{}.pkl'.format(k+1,split_id+1),'rb') as f:\n",
    "            [ink] = pickle.load(f)\n",
    "            true_ids = testinds[split_id]\n",
    "            split = list(ink)\n",
    "            split.pop(0)\n",
    "            metric_list = [metric for metric in split]\n",
    "            [npar,nstimfrom,nstimto] = list(split[0].shape)\n",
    "            for participant in range(npar):\n",
    "                for stimfrom in range(nstimfrom):\n",
    "                    for stimto in range(nstimto):\n",
    "                        pred_list = [metric[participant,stimfrom,stimto] for metric in split]\n",
    "                        [npredfrom, npredto] = list(pred_list[0].shape)\n",
    "                        for predfrom in range(npredfrom):\n",
    "                            for predto in range(npredto):\n",
    "                                single_row = [k+1,true_ids[participant], stimfrom, stimto, predfrom,predto]\n",
    "                                for i in range(len(pred_list)):\n",
    "                                    if i == (len(pred_list) - 1): # turns out MixtureSameFamily only returns the total log prob of evertying in the distribution object, so there's no by-item lp for soft (or dimensional)\n",
    "                                        single_row.append(pred_list[i].item()/(npredfrom*npredto)) #spread the total logprob across cells to make summing later easier\n",
    "                                    else:    \n",
    "                                        single_row.append(pred_list[i][predfrom,predto].item())\n",
    "                                allrows.append(np.array(single_row))\n",
    "    del ink                           \n",
    "allpred = np.stack(allrows,axis = 0)\n",
    "dfgrp = pd.DataFrame(allpred, columns = ['K','pid','stimfrom','stimto','predfrom','predto','d_ae','d_se','d_lpX'])\n",
    "dfgrp.to_csv('kfold_metrics_dim.csv',index = False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
